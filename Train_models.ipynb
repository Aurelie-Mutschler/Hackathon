{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the evolution of creatinine rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import pandas as pd\n",
    "#import time\n",
    "#import copy\n",
    "#from pprint import pprint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, classification_report\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score, train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "#from sklearn.metrics import fbeta_score, make_scorer, classification_report, confusion_matrix\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "from termcolor import colored, cprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of examples :  36251\n",
      "Initial number of columns :  64\n",
      "\n",
      "Dropping 2 columns that are not features : \n",
      "['subject_id', 'icustay_id']\n",
      "After dropping ids, number of columns :  62\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creatinine</th>\n",
       "      <th>age</th>\n",
       "      <th>arterial_pressure_systolic</th>\n",
       "      <th>arterial_pressure_systolic_delay</th>\n",
       "      <th>arterial_pressure_diastolic</th>\n",
       "      <th>arterial_pressure_diastolic_delay</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>heart_rate_delay</th>\n",
       "      <th>weight_daily</th>\n",
       "      <th>weight_daily_delay</th>\n",
       "      <th>...</th>\n",
       "      <th>bilirubin</th>\n",
       "      <th>bilirubin_delay</th>\n",
       "      <th>c_reactive_protein</th>\n",
       "      <th>c_reactive_protein_delay</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>creatinine_yesterday</th>\n",
       "      <th>creatinine_before_yesterday</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>58.363217</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>73380.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>INTRACRANIAL HEMORRHAGE</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>52.688716</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>83700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>S/P BOATING ACCIDENT</td>\n",
       "      <td>M</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>67.864024</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>4260.0</td>\n",
       "      <td>115.8</td>\n",
       "      <td>158820.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SEPSIS</td>\n",
       "      <td>M</td>\n",
       "      <td>1.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>72.399244</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>LOWER GASTROINTESTINAL BLEED</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>53.707219</td>\n",
       "      <td>144.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>54.6</td>\n",
       "      <td>102720.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>ALCHOLIC CIRRHOSIS\\EGD ** REMOTE EAST STONEMAN...</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   creatinine        age  arterial_pressure_systolic  \\\n",
       "0         0.6  58.363217                       155.0   \n",
       "1         0.6  52.688716                       165.0   \n",
       "2         2.0  67.864024                        84.0   \n",
       "3         0.7  72.399244                       120.0   \n",
       "4         1.4  53.707219                       144.0   \n",
       "\n",
       "   arterial_pressure_systolic_delay  arterial_pressure_diastolic  \\\n",
       "0                            1800.0                         75.0   \n",
       "1                            1440.0                         75.0   \n",
       "2                            4200.0                         50.0   \n",
       "3                              60.0                         74.0   \n",
       "4                             900.0                         75.0   \n",
       "\n",
       "   arterial_pressure_diastolic_delay  heart_rate  heart_rate_delay  \\\n",
       "0                             1800.0        58.0            1800.0   \n",
       "1                             1440.0       102.0            1440.0   \n",
       "2                             4200.0       104.0            4260.0   \n",
       "3                               60.0       103.0              60.0   \n",
       "4                              900.0        67.0             900.0   \n",
       "\n",
       "   weight_daily  weight_daily_delay  ...    bilirubin  bilirubin_delay  \\\n",
       "0          93.7             73380.0  ...          NaN              NaN   \n",
       "1         110.0             83700.0  ...          NaN              NaN   \n",
       "2         115.8            158820.0  ...          NaN              NaN   \n",
       "3           NaN                 NaN  ...          NaN              NaN   \n",
       "4          54.6            102720.0  ...          NaN              NaN   \n",
       "\n",
       "   c_reactive_protein  c_reactive_protein_delay  ethnicity  \\\n",
       "0                 NaN                       NaN      WHITE   \n",
       "1                 NaN                       NaN      WHITE   \n",
       "2                 NaN                       NaN      WHITE   \n",
       "3                 NaN                       NaN      WHITE   \n",
       "4                 NaN                       NaN      WHITE   \n",
       "\n",
       "                                           diagnosis  gender  \\\n",
       "0                            INTRACRANIAL HEMORRHAGE       M   \n",
       "1                               S/P BOATING ACCIDENT       M   \n",
       "2                                             SEPSIS       M   \n",
       "3                       LOWER GASTROINTESTINAL BLEED       M   \n",
       "4  ALCHOLIC CIRRHOSIS\\EGD ** REMOTE EAST STONEMAN...       M   \n",
       "\n",
       "   creatinine_yesterday  creatinine_before_yesterday  label  \n",
       "0                   NaN                          0.7    2.0  \n",
       "1                   0.6                          0.6    2.0  \n",
       "2                   1.9                          NaN    1.0  \n",
       "3                   NaN                          NaN    2.0  \n",
       "4                   NaN                          1.3    2.0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"dataset_with_labels.csv\", engine='python').drop('Unnamed: 0',axis=1).reset_index(drop=True)\n",
    "print('Initial number of examples : ', data.shape[0])\n",
    "print('Initial number of columns : ', data.shape[1])\n",
    "print()\n",
    "\n",
    "# Remove columns that are ids\n",
    "id_cols = [c for c in data.columns.values if ('_id' in c)]\n",
    "print('Dropping ' + str(len(id_cols)) + ' columns that are not features : ')\n",
    "print(id_cols)\n",
    "for c in id_cols:\n",
    "    data = data.drop(c,axis=1)\n",
    "print('After dropping ids, number of columns : ', data.shape[1])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following columns have missing rate >30% and will be dropped : \n",
      "['weight_daily' 'weight_daily_delay' 'urine_output' 'urine_output_delay'\n",
      " 'day_urine_output' 'day_urine_output_delay' 'scr' 'scr_delay' 'sodium'\n",
      " 'sodium_delay' 'potassium' 'potassium_delay' 'calcium' 'calcium_delay'\n",
      " 'phosphor' 'phosphor_delay' 'hemoglobine' 'hemoglobine_delay' 'uric_acid'\n",
      " 'uric_acid_delay' 'chloride' 'chloride_delay' 'platelet_count'\n",
      " 'platelet_count_delay' 'fibrinogen' 'fibrinogen_delay' 'urinary_sodium'\n",
      " 'urinary_sodium_delay' 'urinary_potassium' 'urinary_potassium_delay'\n",
      " 'urine_creatinin' 'urine_creatinin_delay' 'alkaline_phospatase'\n",
      " 'alkaline_phospatase_delay' 'total_protein_blood'\n",
      " 'total_protein_blood_delay' 'albumin' 'albumin_delay'\n",
      " 'total_protein_urine' 'total_protein_urine_delay' 'bilirubin'\n",
      " 'bilirubin_delay' 'c_reactive_protein' 'c_reactive_protein_delay'\n",
      " 'creatinine_yesterday' 'creatinine_before_yesterday']\n",
      "Number of columns kept :  16\n",
      "['creatinine' 'age' 'arterial_pressure_systolic'\n",
      " 'arterial_pressure_systolic_delay' 'arterial_pressure_diastolic'\n",
      " 'arterial_pressure_diastolic_delay' 'heart_rate' 'heart_rate_delay'\n",
      " 'temperature' 'temperature_delay' 'ph_blood' 'ph_blood_delay' 'ethnicity'\n",
      " 'diagnosis' 'gender' 'label']\n",
      "\n",
      "Drop lines where there are some NAs remaining...\n",
      "Number of lines kept :  28127\n",
      "\n",
      "Number of missing values remaining :  0\n"
     ]
    }
   ],
   "source": [
    "# Throw features that have a rate of missing values > x% \n",
    "max_missing_rate = 0.3\n",
    "missing_rates = data.apply(lambda x : x.isnull(), axis=1).sum(axis=0)/data.shape[0]\n",
    "to_drop = missing_rates[missing_rates>0.3].index.values\n",
    "\n",
    "print('Following columns have missing rate >' + str(int(100*max_missing_rate)) + '% and will be dropped : ')\n",
    "print(to_drop)\n",
    "data = data.drop(labels=to_drop,axis=1)\n",
    "print('Number of columns kept : ', data.shape[1])\n",
    "print(data.columns.values)\n",
    "print()\n",
    "\n",
    "# Then drop lines where there remain some missing values\n",
    "print('Drop lines where there are some NAs remaining...')\n",
    "data = data.dropna(axis=0,how='any').reset_index(drop=True)\n",
    "print('Number of lines kept : ', data.shape[0])\n",
    "print()\n",
    "\n",
    "# Check there is not missing value left in dataset\n",
    "print('Number of missing values remaining : ', data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping modalities of ethnicity towards simplified modalities...\n",
      "Remaining values for ethnicity : \n",
      "WHITE      20968\n",
      "UNKNOWN     2565\n",
      "BLACK       2111\n",
      "OTHER       1787\n",
      "ASIAN        654\n",
      "NaN           42\n",
      "Name: ethnicity, dtype: int64\n",
      "\n",
      "Mapping modalities of diagnosis towards simplified modalities...\n",
      "Remaining values of diagnosis : \n",
      "OTHER                       20856\n",
      "SEPSIS                       1280\n",
      "PNEUMONIA                    1169\n",
      "INTRACRANIAL HEMORRHAGE       848\n",
      "RESPIRATORY FAILURE           644\n",
      "SUBARACHNOID HEMORRHAGE       637\n",
      "ALTERED MENTAL STATUS         487\n",
      "ABDOMINAL PAIN                402\n",
      "PANCREATITIS                  383\n",
      "CHEST PAIN                    328\n",
      "CORONARY ARTERY DISEASE       295\n",
      "CONGESTIVE HEART FAILURE      273\n",
      "HYPOTENSION                   216\n",
      "GASTROINTESTINAL BLEED        179\n",
      "ACUTE RENAL FAILURE           130\n",
      "Name: diagnosis, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Before performing dummy encoding, map some categorical variables to fewer modalities\n",
    "# This is to control the number of features\n",
    "print('Mapping modalities of ethnicity towards simplified modalities...')\n",
    "simple_ethnicity = {\n",
    "   'BLACK/AFRICAN AMERICAN': 'BLACK', \n",
    "   'WHITE': 'WHITE', \n",
    "   'UNKNOWN/NOT SPECIFIED': 'UNKNOWN',\n",
    "   'HISPANIC/LATINO - DOMINICAN': 'OTHER', \n",
    "   'UNABLE TO OBTAIN': 'UNKNOWN',\n",
    "   'PATIENT DECLINED TO ANSWER': 'UNKNOWN', \n",
    "   'ASIAN - CHINESE': 'ASIAN',\n",
    "   'AMERICAN INDIAN/ALASKA NATIVE': 'OTHER', \n",
    "   'MULTI RACE ETHNICITY': 'OTHER',\n",
    "   'WHITE - OTHER EUROPEAN': 'WHITE', \n",
    "   'OTHER': 'OTHER', \n",
    "   'PORTUGUESE': 'WHITE',\n",
    "   'HISPANIC OR LATINO': 'OTHER', \n",
    "   'ASIAN': 'ASIAN', \n",
    "   'HISPANIC/LATINO - PUERTO RICAN': 'OTHER',\n",
    "   'MIDDLE EASTERN': 'OTHER', \n",
    "   'ASIAN - KOREAN': 'ASIAN', \n",
    "   'BLACK/HAITIAN': 'BLACK',\n",
    "   'ASIAN - OTHER': 'ASIAN', \n",
    "   'HISPANIC/LATINO - CUBAN': 'OTHER', \n",
    "   'ASIAN - FILIPINO': 'ASIAN',\n",
    "   'BLACK/CAPE VERDEAN': 'BLACK', \n",
    "   'WHITE - BRAZILIAN': 'WHITE', \n",
    "   'ASIAN - ASIAN INDIAN': 'ASIAN',\n",
    "   'WHITE - EASTERN EUROPEAN': 'WHITE', \n",
    "   'HISPANIC/LATINO - GUATEMALAN': 'OTHER',\n",
    "   'ASIAN - VIETNAMESE': 'ASIAN', \n",
    "   'HISPANIC/LATINO - MEXICAN': 'OTHER',\n",
    "   'WHITE - RUSSIAN': 'WHITE', \n",
    "   'BLACK/AFRICAN': 'BLACK', \n",
    "   'ASIAN - CAMBODIAN': 'ASIAN',\n",
    "   'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE': 'OTHER'\n",
    "}\n",
    "\n",
    "data.loc[:,'ethnicity'] = data['ethnicity'].map(simple_ethnicity)\n",
    "print('Remaining values for ethnicity : ')\n",
    "print(data['ethnicity'].value_counts(dropna=False))\n",
    "print()\n",
    "\n",
    "print('Mapping modalities of diagnosis towards simplified modalities...')\n",
    "to_sepsis = data['diagnosis'].apply(lambda x : 'FEVER' in x)\n",
    "data.loc[to_sepsis,'diagnosis'] = 'SEPSIS'\n",
    "\n",
    "to_resp_failure = data['diagnosis'].apply(lambda x : (('DYSPNEA' in x) | ('SHORTNESS OF BREATH' in x)))\n",
    "data.loc[to_resp_failure,'diagnosis'] = 'RESPIRATORY FAILURE'\n",
    "\n",
    "diag_to_keep = ['PNEUMONIA', 'CONGESTIVE HEART FAILURE', 'SUBARACHNOID HEMORRHAGE',\n",
    "              'INTRACRANIAL HEMORRHAGE', 'ALTERED MENTAL STATUS', 'CORONARY ARTERY DISEASE',\n",
    "              'ABDOMINAL PAIN', 'CHEST PAIN', 'HYPOTENSION', 'ACUTE RENAL FAILURE',\n",
    "              'RESPIRATORY FAILURE', 'GASTROINTESTINAL BLEED', 'PANCREATITIS', 'SEPSIS']\n",
    "to_other = data['diagnosis'].apply(lambda x : x not in diag_to_keep)\n",
    "data.loc[to_other, 'diagnosis'] = 'OTHER'\n",
    "\n",
    "print('Remaining values of diagnosis : ')\n",
    "print(data['diagnosis'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dummy encoding for features : \n",
      "['ethnicity', 'diagnosis', 'gender']\n",
      "New list of columns :\n",
      "['creatinine' 'age' 'arterial_pressure_systolic'\n",
      " 'arterial_pressure_systolic_delay' 'arterial_pressure_diastolic'\n",
      " 'arterial_pressure_diastolic_delay' 'heart_rate' 'heart_rate_delay'\n",
      " 'temperature' 'temperature_delay' 'ph_blood' 'ph_blood_delay' 'label'\n",
      " 'ethnicity_ASIAN' 'ethnicity_BLACK' 'ethnicity_OTHER' 'ethnicity_UNKNOWN'\n",
      " 'ethnicity_WHITE' 'diagnosis_ABDOMINAL PAIN'\n",
      " 'diagnosis_ACUTE RENAL FAILURE' 'diagnosis_ALTERED MENTAL STATUS'\n",
      " 'diagnosis_CHEST PAIN' 'diagnosis_CONGESTIVE HEART FAILURE'\n",
      " 'diagnosis_CORONARY ARTERY DISEASE' 'diagnosis_GASTROINTESTINAL BLEED'\n",
      " 'diagnosis_HYPOTENSION' 'diagnosis_INTRACRANIAL HEMORRHAGE'\n",
      " 'diagnosis_OTHER' 'diagnosis_PANCREATITIS' 'diagnosis_PNEUMONIA'\n",
      " 'diagnosis_RESPIRATORY FAILURE' 'diagnosis_SEPSIS'\n",
      " 'diagnosis_SUBARACHNOID HEMORRHAGE' 'gender_F' 'gender_M']\n"
     ]
    }
   ],
   "source": [
    "# Perform dummy encoding\n",
    "dummy_variables = ['ethnicity','diagnosis','gender']\n",
    "print('Performing dummy encoding for features : ')\n",
    "print(dummy_variables)\n",
    "data = pd.get_dummies(data,columns=dummy_variables)\n",
    "print('New list of columns :')\n",
    "print(data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of examples :  28127\n",
      "Dropping outliers for feature :  creatinine\n",
      "Remaining number of examples :  28122\n",
      "Dropping outliers for feature :  ph_blood\n",
      "Remaining number of examples :  28122\n",
      "Dropping outliers for feature :  age\n",
      "Remaining number of examples :  27313\n"
     ]
    }
   ],
   "source": [
    "feature_ranges = {\n",
    "    'creatinine': {'min': 0.0, 'max': 20.0},\n",
    "    'creatinine_yesterday': {'min': 0.0, 'max': 20.0},\n",
    "    'creatinine_before_yesterday': {'min': 0.0, 'max': 20.0},\n",
    "    'potassium': {'min': 1.2, 'max': 7.0},\n",
    "    'ph_blood': {'min': 0.0, 'max': 14.0},\n",
    "    'age': {'min': 0.0, 'max': 110.0},\n",
    "    'bilirubin': {'min': 0.0, 'max': 20.0}\n",
    "}\n",
    "\n",
    "print('Initial number of examples : ', data.shape[0])\n",
    "for k in feature_ranges:\n",
    "    if k in data.columns.values:\n",
    "        print('Dropping outliers for feature : ', k)\n",
    "        data = data.loc[((data[k]>feature_ranges[k]['min']) & (data[k]<feature_ranges[k]['max'])),:]\n",
    "        print('Remaining number of examples : ', data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing following features :  ['creatinine', 'age', 'arterial_pressure_systolic', 'arterial_pressure_diastolic', 'heart_rate', 'temperature', 'ph_blood']\n",
      "--- Feature :  creatinine\n",
      "mean    1.264320e-16\n",
      "std     1.000018e+00\n",
      "Name: creatinine, dtype: float64\n",
      "--- Feature :  age\n",
      "mean   -4.531781e-16\n",
      "std     1.000018e+00\n",
      "Name: age, dtype: float64\n",
      "--- Feature :  arterial_pressure_systolic\n",
      "mean    6.243556e-17\n",
      "std     1.000018e+00\n",
      "Name: arterial_pressure_systolic, dtype: float64\n",
      "--- Feature :  arterial_pressure_diastolic\n",
      "mean    1.248711e-17\n",
      "std     1.000018e+00\n",
      "Name: arterial_pressure_diastolic, dtype: float64\n",
      "--- Feature :  heart_rate\n",
      "mean   -3.080155e-16\n",
      "std     1.000018e+00\n",
      "Name: heart_rate, dtype: float64\n",
      "--- Feature :  temperature\n",
      "mean    1.601472e-15\n",
      "std     1.000018e+00\n",
      "Name: temperature, dtype: float64\n",
      "--- Feature :  ph_blood\n",
      "mean    1.673728e-15\n",
      "std     1.000018e+00\n",
      "Name: ph_blood, dtype: float64\n",
      "\n",
      "Standardizing following features :  ['arterial_pressure_systolic_delay', 'arterial_pressure_diastolic_delay', 'heart_rate_delay', 'temperature_delay', 'ph_blood_delay']\n",
      "--- Feature :  arterial_pressure_systolic_delay\n",
      "mean   -0.210023\n",
      "std     0.034534\n",
      "Name: arterial_pressure_systolic_delay, dtype: float64\n",
      "--- Feature :  arterial_pressure_diastolic_delay\n",
      "mean   -0.210001\n",
      "std     0.034561\n",
      "Name: arterial_pressure_diastolic_delay, dtype: float64\n",
      "--- Feature :  heart_rate_delay\n",
      "mean   -0.210350\n",
      "std     0.028816\n",
      "Name: heart_rate_delay, dtype: float64\n",
      "--- Feature :  temperature_delay\n",
      "mean   -0.077984\n",
      "std     0.517715\n",
      "Name: temperature_delay, dtype: float64\n",
      "--- Feature :  ph_blood_delay\n",
      "mean    0.708358\n",
      "std     2.022030\n",
      "Name: ph_blood_delay, dtype: float64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a sklearn StandardScaler() for all numeric variables\n",
    "numeric_variables = ['creatinine', 'age', 'arterial_pressure_systolic', 'arterial_pressure_diastolic',\n",
    "                     'heart_rate', 'temperature', 'ph_blood']\n",
    "print('Standardizing following features : ', numeric_variables)\n",
    "numeric_scaler = StandardScaler()\n",
    "data.loc[:,numeric_variables] = numeric_scaler.fit_transform(data.loc[:,numeric_variables])\n",
    "for c,col in (data.loc[:,numeric_variables].describe(percentiles=[]).loc[['mean','std'],:]).iteritems():\n",
    "    print('--- Feature : ', c)\n",
    "    print(col)\n",
    "print()\n",
    "\n",
    "# Special case for delays : as they are all times in seconds, create one unique scaler for all delays\n",
    "# (i.e. apply same transformation to each time measurement, such that they remain comparable)\n",
    "time_variables = [i for i in data.columns.values if '_delay' in i]\n",
    "print('Standardizing following features : ', time_variables)\n",
    "time_scaler = StandardScaler()\n",
    "time_scaler.fit(data.loc[:,time_variables].values.flatten())\n",
    "data.loc[:,time_variables] = time_scaler.transform(data.loc[:,time_variables])\n",
    "\n",
    "for c,col in (data.loc[:,time_variables].describe(percentiles=[]).loc[['mean','std'],:]).iteritems():\n",
    "    print('--- Feature : ', c)\n",
    "    print(col)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare X and y input datasets\n",
    "Split dataset into three sets :\n",
    "- **hyperopt set (80%)** : for each model, perform grid search with cross-validation for hyperparameter optimization.\n",
    "- **compare set (10%)** : compare performances obtained on this set for each model (with optimized hyperparameters)\n",
    "- **test set (10%)** : in the end, check for overfitting by comparing the change in performances between compare_set and test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hyperopt set :\n",
      "Number of examples :  21850\n",
      "Class balance : \n",
      "2.0    13337\n",
      "1.0     4702\n",
      "0.0     3811\n",
      "Name: label, dtype: int64\n",
      "--- Compare set :\n",
      "Number of examples :  2731\n",
      "Class balance : \n",
      "2.0    1667\n",
      "1.0     588\n",
      "0.0     476\n",
      "Name: label, dtype: int64\n",
      "--- Test set :\n",
      "Number of examples :  2732\n",
      "Class balance : \n",
      "2.0    1667\n",
      "1.0     588\n",
      "0.0     477\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate label from features\n",
    "y = data['label']\n",
    "X = data.drop('label',axis=1)\n",
    "\n",
    "# Split into hyperopt, compare and test sets\n",
    "X_hyperopt, X1, y_hyperopt, y1 = train_test_split(X, y, train_size=0.8, stratify=y.values)\n",
    "X_compare, X_test, y_compare, y_test = train_test_split(X1, y1, train_size=0.5, stratify=y1.values)\n",
    "\n",
    "print('--- Hyperopt set :')\n",
    "print('Number of examples : ', X_hyperopt.shape[0])\n",
    "print('Class balance : ')\n",
    "print(y_hyperopt.value_counts(dropna=False))\n",
    "print('--- Compare set :')\n",
    "print('Number of examples : ', X_compare.shape[0])\n",
    "print('Class balance : ')\n",
    "print(y_compare.value_counts(dropna=False))\n",
    "print('--- Test set :')\n",
    "print('Number of examples : ', X_test.shape[0])\n",
    "print('Class balance : ')\n",
    "print(y_test.value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize models and their respective set of hyperparameters for grid search\n",
    "- Create list of dict : [{**'model_name'**: sklearn_model, **'hyperparam_grid'**:{**'hyperparam_name'**:[list of possible values]}}]\n",
    "- then just loop on this list to perform hyperparameter optim for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers and their respective grids for hyperparameter tuning should be specified here :\n",
    "models = {\n",
    "    'linear_SVM': {\n",
    "        'classifier': OneVsRestClassifier(SGDClassifier(loss='hinge', penalty='l2', learning_rate='optimal', \n",
    "                                                        class_weight='balanced'), n_jobs=-1),\n",
    "        'search_grid': {\n",
    "            'estimator__alpha': [0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001, 0.003, 0.1, 0.3, 1.0]   \n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'classifier': OneVsRestClassifier(SGDClassifier(loss='log', penalty='l2', learning_rate='optimal', \n",
    "                                                        class_weight='balanced'), n_jobs=-1),\n",
    "        'search_grid': {\n",
    "            'estimator__alpha': [0.000001,0.000003,0.00001,0.00003,0.0001,0.0003,0.001, 0.003, 0.1, 0.3, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression_elasticnet': {\n",
    "        'classifier': OneVsRestClassifier(SGDClassifier(loss='log', penalty='elasticnet', learning_rate='optimal',\n",
    "                                                        class_weight='balanced'), n_jobs=-1),\n",
    "        'search_grid': {\n",
    "            'estimator__alpha': [0.000001,0.000003,0.00001,0.00005,0.0001,0.0005,0.001],\n",
    "            'estimator__l1_ratio': [0.15,0.3,0.6,1.0]\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'classifier': RandomForestClassifier(n_estimators=10, max_depth=None),\n",
    "        'search_grid': {\n",
    "            'n_estimators': [int(x) for x in np.linspace(200,2000,10)],\n",
    "            'max_depth': np.append([int(x) for x in np.linspace(10,100,10)], None)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for each model\n",
    "/!\\ this is time-consuming, pay attention to save the intermediate results of the grid search in a file\n",
    "- in the end, create a list containing the best classifier obtained for each model : [sklearn_model(best hyperparams), kerasclassifier(best_hyperparams),...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom performance metrics that will be used for hyperparameter optimization\n",
    "def sensitivity_increase(y,y_pred):\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    return conf_mat[0,0]/(conf_mat[0,0] + conf_mat[0,1] + conf_mat[0,2])\n",
    "\n",
    "def specificity_decrease(y,y_pred):\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    return (conf_mat[0,0] + conf_mat[0,2] + conf_mat[2,0] + conf_mat[2,2])/(conf_mat[0,0] + conf_mat[0,2] \n",
    "                                                                            + conf_mat[2,0] + conf_mat[2,2]\n",
    "                                                                           + conf_mat[0,1] + conf_mat[2,1])\n",
    "\n",
    "# The dictionnary below can be used in GridSearchCV() for multiple scoring\n",
    "# /!\\ This is doable only with sklearn 0.19 (current stable release: 0.18)\n",
    "# For now we use only sensitivity_increase for hyperparameter optimization\n",
    "scores = {\n",
    "    'sensitivity_increase': sensitivity_increase, #make_scorer(sensitivity_increase),\n",
    "    'specificity_decrease': specificity_decrease#make_scorer(specificity_decrease)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Grid search for linear_SVM ---\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score :  0.32326838848777323\n",
      "Best set of hyperparameters :  {'estimator__alpha': 3e-06}\n",
      "---- Grid search for logistic_regression ---\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   11.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score :  0.3523860636469389\n",
      "Best set of hyperparameters :  {'estimator__alpha': 3e-06}\n",
      "---- Grid search for logistic_regression_elasticnet ---\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:   28.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score :  0.3503124740986324\n",
      "Best set of hyperparameters :  {'estimator__alpha': 1e-05, 'estimator__l1_ratio': 1.0}\n",
      "---- Grid search for random_forest ---\n",
      "Fitting 3 folds for each of 110 candidates, totalling 330 fits\n"
     ]
    }
   ],
   "source": [
    "# Loop over each model and perform grid search to tune its hyperparameters\n",
    "for i in models.keys():\n",
    "    print('---- Grid search for ' + str(i) + ' ---')\n",
    "    clf = models[i]['classifier']\n",
    "    params = models[i]['search_grid']\n",
    "    \n",
    "    #gridsearch = GridSearchCV(estimator=clf, param_grid=params, refit=True, verbose=1)\n",
    "    gridsearch = GridSearchCV(estimator=clf, param_grid=params, scoring=make_scorer(sensitivity_increase), refit=True, verbose=1)\n",
    "    gridsearch.fit(X_hyperopt,y_hyperopt)\n",
    "    models[i]['best_classifier'] = gridsearch.best_estimator_\n",
    "    models[i]['best_scores'] = gridsearch.best_score_\n",
    "    models[i]['best_params'] = gridsearch.best_params_\n",
    "    print('Best score : ', gridsearch.best_score_)\n",
    "    print('Best set of hyperparameters : ', gridsearch.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison of performances\n",
    "Re-train models with their respective optimal sets of hyperparameters, on ALL the \"hyperopt\" dataset. Then assess and compare performances on the \"compare\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_performances = pd.DataFrame(index=list(models.keys()),columns=list(scores.keys()))\n",
    "for k in list(models.keys()):\n",
    "    clf = models[k]['best_classifier']\n",
    "    # Use ALL examples from \"hyperopt\" dataset to re-train model\n",
    "    # NB: this is not mandatory as GridSearchCV(refit=True) does it.\n",
    "    clf.fit(X_hyperopt,y_hyperopt)\n",
    "    # Assess performances on \"compare\" dataset\n",
    "    y_pred = clf.predict(X_compare)   \n",
    "    for m in list(scores.keys()):\n",
    "        compare_performances.loc[k,m] = scores[m](y_compare, y_pred)\n",
    "print(compare_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models['random_forest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Check for overfitting\n",
    "Re-train models on ALL the examples from \"hyperopt\" and \"compare\", then assess performances on the \"test\" dataset.\n",
    "NB : if there's a significant discrepancy in performances between \"compare\" and \"test\" datasets, we may be overfitting or suffering from bias. The generalized performances are the ones obtained on the \"test\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_performances = pd.DataFrame(index=list(models.keys()),columns=list(scores.keys()))\n",
    "for k in list(models.keys()):\n",
    "    clf = models[k]['best_classifier']\n",
    "    # Use ALL examples from \"hyperopt\" AND \"compare\" datasets to re-train models\n",
    "    clf.fit(X_hyperopt.append(X_compare),y_hyperopt.append(y_compare))\n",
    "    # Assess performances on \"test\" dataset\n",
    "    y_pred = clf.predict(X_test)   \n",
    "    for m in list(scores.keys()):\n",
    "        test_performances.loc[k,m] = scores[m](y_test, y_pred)\n",
    "        \n",
    "print('Performances on \"compare\" set : ')\n",
    "print(compare_performances)\n",
    "print()\n",
    "print('Performances on \"test\" set (= generalized) : ')\n",
    "print(test_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alternative to be tested : Don't drop any missing values from the dataset but rather train an XGBoost classifier that handles missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
